{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PD5: Click-Through Rate Prediction\n",
    "## MTI850 - Big Data Analytics\n",
    "### Fall 2024\n",
    "### Rev 1.0 Fall 2025 - Spark 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>    \n",
    "    \n",
    "| Équipe   | #                                                       |\n",
    "|----------|---------------------------------------------------------|\n",
    "| Nom1     | Code permanent1|\n",
    "| Nom2     | Code permanent2|\n",
    "| Nom3     | Code permanent3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Spark Logo](http://alekoe.github.io/images/ta_Spark-logo-small.png) ![Python Logo](http://alekoe.github.io/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "\n",
    "\n",
    "This assignment covers the steps for creating a click-through rate (CTR) prediction pipeline. You will work with the [Criteo Labs](http://labs.criteo.com/) dataset that was used for a [Kaggle competition](https://www.kaggle.com/c/criteo-display-ad-challenge).\n",
    "\n",
    "This assignment covers:\n",
    "\n",
    "* *Part 1:* Featurize categorical data using one-hot-encoding (OHE)\n",
    "\n",
    "* *Part 2:* Construct an OHE dictionary\n",
    "\n",
    "* *Part 3:* Parse CTR data and generate OHE features\n",
    "   * *Visualization 1:* Feature frequency<br><br>\n",
    "\n",
    "* *Part 4:* CTR prediction and logloss evaluation\n",
    "   * *Visualization 2:* ROC curve<br><br>\n",
    "\n",
    "* *Part 5:* Reduce feature dimension via feature hashing<br><br><br>\n",
    "\n",
    "> Note that, for reference, you can look up the details of: \n",
    "> * the relevant Spark methods in [PySpark's DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/index.html)\n",
    "> * the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)\n",
    "\n",
    "<br><br>\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"> <img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\"/> </a>This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"> Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Preparing the environment\n",
    "\n",
    "By default, when a shuffle operation occurs with DataFrames, the post-shuffle partition count is 200. This is controlled by Spark configuration value ``spark.sql.shuffle.partitions``. 200 is a little too high for this dataset, so we set the post-shuffle partition count to twice the number of available cores in commodity machines. We are setting it to 6.\n",
    "\n",
    "shuffle = données redistribuées ente les partitions --> une des opérations les plus couteuses dans spark\n",
    "Apparait quand : regroupement des données selon une clé / tri des données / jointure des tables par clé / calcul d'un groupby ou reduce / créer un dictionnaire OHE etc ... \n",
    "ici on change la size du shuffle car trop de répartitions en sortie --> inutilement lent pour notre dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Test module for MTI850\n",
    "import testmti850\n",
    "\n",
    "# Util module for MTI850\n",
    "import utilmti850\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# on force spark a utiliser 6 paritions après le shuffle\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local\") \\\n",
    ".appName(\"Click Rate Prediction\") \\\n",
    ".config(\"spark.sql.shuffle.partitions\", \"6\") \\\n",
    ".config(\"spark.driver.maxResultSize\", \"10g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether the ``spark.sql.shuffle.partitions`` was properly set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.rdd.compress', 'True'),\n",
       " ('spark.app.startTime', '1762620563270'),\n",
       " ('spark.hadoop.fs.s3a.vectored.read.min.seek.size', '128K'),\n",
       " ('spark.master', 'local'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.sql.artifact.isolation.enabled', 'false'),\n",
       " ('spark.sql.warehouse.dir', 'file:/home/vboxuser/MTI850-PDs/spark-warehouse'),\n",
       " ('spark.app.id', 'local-1762620565749'),\n",
       " ('spark.app.submitTime', '1762620561611'),\n",
       " ('spark.driver.port', '41101'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.hadoop.fs.s3a.vectored.read.max.merged.size', '2M'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', 'MTI850'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.sql.shuffle.partitions', '6'),\n",
       " ('spark.app.name', 'Click Rate Prediction'),\n",
       " ('spark.driver.maxResultSize', '10g'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current value of spark.sql.shuffle.partitions is: 6\n"
     ]
    }
   ],
   "source": [
    "# or\n",
    "current_value = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(f\"The current value of spark.sql.shuffle.partitions is: {current_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Featurize categorical data using one-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) One-hot-encoding\n",
    "\n",
    "We would like to develop code to convert categorical features to numerical ones, and to build intuition, we will work with a sample unlabeled dataset with three data points, with each data point representing an animal.\n",
    "\n",
    "* The first feature indicates the type of animal (bear, cat, mouse);\n",
    "\n",
    "* The second feature describes the animal's color (black, tabby); and\n",
    "\n",
    "* The third (optional) feature describes what the animal eats (mouse, salmon).\n",
    "\n",
    "In a one-hot-encoding (OHE) scheme, we want to represent each tuple of `(featureID, category)` via its own binary feature.  We can do this in Python by creating a dictionary that maps each tuple to a distinct integer, where the integer corresponds to a binary feature.\n",
    "\n",
    "To start, manually enter the entries in the OHE dictionary associated with the sample dataset by mapping the tuples to consecutive integers starting from zero, ordering the tuples first by featureID and next by category.\n",
    "\n",
    "Later in this assignement, you will use OHE dictionaries to transform data points into compact lists of features that can be used in machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+\n",
      "|animal|color|  food|\n",
      "+------+-----+------+\n",
      "| mouse|black|  NULL|\n",
      "|   cat|tabby| mouse|\n",
      "|  bear|black|salmon|\n",
      "+------+-----+------+\n",
      "\n",
      "+------------------------------------+\n",
      "|features                            |\n",
      "+------------------------------------+\n",
      "|[{0, mouse}, {1, black}]            |\n",
      "|[{0, cat}, {1, tabby}, {2, mouse}]  |\n",
      "|[{0, bear}, {1, black}, {2, salmon}]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Data for manual OHE\n",
    "# Note: the first data point does not include any value for the optional third feature (animal eats)\n",
    "sample_one = [(0, 'mouse'), (1, 'black')]\n",
    "sample_two = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "sample_three =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]\n",
    "\n",
    "def sample_to_row(sample):\n",
    "    tmp_dict = defaultdict(lambda: None) # init a non-existing key None if it is required\n",
    "    tmp_dict.update(sample)\n",
    "    return [tmp_dict[i] for i in range(3)] # using range, it is possible to return the items ordered\n",
    "\n",
    "spark.createDataFrame(\n",
    "    map(sample_to_row, [sample_one, sample_two, sample_three]), # features\n",
    "    ['animal', 'color', 'food']                                 # features/columns names \n",
    ").show()\n",
    "\n",
    "sample_data_df = spark.createDataFrame([(sample_one,), (sample_two,), (sample_three,)], ['features'])\n",
    "\n",
    "sample_data_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 'bear'): 0, (0, 'cat'): 1, (0, 'mouse'): 2, (1, 'black'): 3, (1, 'tabby'): 4, (2, 'mouse'): 5, (2, 'salmon'): 6}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "sample_ohe_dict_manual = {}\n",
    "sample_ohe_dict_manual[(0, 'bear')] = 0\n",
    "sample_ohe_dict_manual[(0, 'cat')] = 1\n",
    "sample_ohe_dict_manual[(0, 'mouse')] = 2\n",
    "sample_ohe_dict_manual[(1, 'black')] = 3\n",
    "sample_ohe_dict_manual[(1, 'tabby')] = 4\n",
    "sample_ohe_dict_manual[(2, 'mouse')] = 5\n",
    "sample_ohe_dict_manual[(2, 'salmon')] = 6\n",
    "\n",
    "print(sample_ohe_dict_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST One-hot-encoding (1a)\n",
    "testmti850.Test.assertEqualsHashed(sample_ohe_dict_manual[(0, 'bear')],\n",
    "                        'b6589fc6ab0dc82cf12099d1c2d40ab994e8410c',\n",
    "                        \"incorrect value for sample_ohe_dict_manual[(0,'bear')]\")\n",
    "testmti850.Test.assertEqualsHashed(sample_ohe_dict_manual[(0, 'cat')],\n",
    "                        '356a192b7913b04c54574d18c28d46e6395428ab',\n",
    "                        \"incorrect value for sample_ohe_dict_manual[(0,'cat')]\")\n",
    "testmti850.Test.assertEqualsHashed(sample_ohe_dict_manual[(0, 'mouse')],\n",
    "                        'da4b9237bacccdf19c0760cab7aec4a8359010b0',\n",
    "                        \"incorrect value for sample_ohe_dict_manual[(0,'mouse')]\")\n",
    "testmti850.Test.assertEqualsHashed(sample_ohe_dict_manual[(1, 'black')],\n",
    "                        '77de68daecd823babbb58edb1c8e14d7106e83bb',\n",
    "                        \"incorrect value for sample_ohe_dict_manual[(1,'black')]\")\n",
    "testmti850.Test.assertEqualsHashed(sample_ohe_dict_manual[(1, 'tabby')],\n",
    "                        '1b6453892473a467d07372d45eb05abc2031647a',\n",
    "                        \"incorrect value for sample_ohe_dict_manual[(1,'tabby')]\")\n",
    "testmti850.Test.assertEqualsHashed(sample_ohe_dict_manual[(2, 'mouse')],\n",
    "                        'ac3478d69a3c81fa62e60f5c3696165a4e5e6ac4',\n",
    "                        \"incorrect value for sample_ohe_dict_manual[(2,'mouse')]\")\n",
    "testmti850.Test.assertEqualsHashed(sample_ohe_dict_manual[(2, 'salmon')],\n",
    "                        'c1dfd96eea8cc2b62785275bca38ac261256e278',\n",
    "                        \"incorrect value for sample_ohe_dict_manual[(2,'salmon')]\")\n",
    "testmti850.Test.assertEquals(len(sample_ohe_dict_manual.keys()), 7,\n",
    "                  'incorrect number of keys in sample_ohe_dict_manual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Sparse vectors\n",
    "\n",
    "Data points can typically be represented with a small number of non-zero OHE features relative to the total number of features that occur in the dataset.  By leveraging this sparsity and using sparse vector representations for OHE data, we can reduce storage and computational burdens.  Below are a few sample vectors represented as dense numpy arrays.\n",
    "\n",
    "Use [SparseVector](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.linalg.SparseVector.html?highlight=sparse#pyspark.mllib.linalg.SparseVector) to represent them in a sparse fashion, and verify that both the sparse and dense representations yield the same results when computing [dot products](http://en.wikipedia.org/wiki/Dot_product) (we will later use MLlib to train classifiers via gradient descent, and MLlib will need to compute dot products between SparseVectors and dense parameter vectors).\n",
    "\n",
    "Use `SparseVector(size, *args)` to create a new sparse vector where size is the length of the vector and args is either:\n",
    "\n",
    "1. A list of indices and a list of values corresponding to the indices. The indices list must be sorted in ascending order. For example, SparseVector(5, [1, 3, 4], [10, 30, 40]) will represent the vector [0, 10, 0, 30, 40]. The non-zero indices are 1, 3 and 4. On the other hand, SparseVector(3, [2, 1], [5, 5]) will give you an error because the indices list [2, 1] is not in ascending order. Note: you cannot simply sort the indices list, because otherwise the values will not correspond to the respective indices anymore.\n",
    "\n",
    "2. A list of (index, value) pair. In this case, the indices need not be sorted. For example, SparseVector(5, [(3, 1), (1, 2)]) will give you the vector [0, 2, 0, 1, 0].\n",
    "\n",
    "SparseVectors are much more efficient when working with sparse data because they do not store zero values (only store non-zero values and their indices). You'll need to create a sparse vector representation of each dense vector `a_dense` and `b_dense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.ml.linalg import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.300000000000001\n",
      "7.300000000000001\n",
      "-0.5\n",
      "-0.5\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "a_dense = np.array([0., 3., 0., 4.])\n",
    "\n",
    "a_sparse = SparseVector(4,[1,3], [3., 4.])\n",
    "\n",
    "b_dense = np.array([0., 0., 0., 1.])\n",
    "\n",
    "b_sparse = SparseVector(4, [3], [1.]) # 4 elements: only the index 3 element is non-zero (1, in this case)\n",
    "\n",
    "w = np.array([0.4, 3.1, -1.4, -.5])\n",
    "\n",
    "print(a_dense.dot(w)) \n",
    "print(a_sparse.dot(w))\n",
    "print(b_dense.dot(w))\n",
    "print(b_sparse.dot(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Sparse Vectors (1b)\n",
    "testmti850.Test.assertTrue(isinstance(a_sparse, SparseVector), 'a_sparse needs to be an instance of SparseVector')\n",
    "testmti850.Test.assertTrue(isinstance(b_sparse, SparseVector), 'a_sparse needs to be an instance of SparseVector')\n",
    "testmti850.Test.assertTrue(a_dense.dot(w) == a_sparse.dot(w),\n",
    "                'dot product of a_dense and w should equal dot product of a_sparse and w')\n",
    "testmti850.Test.assertTrue(b_dense.dot(w) == b_sparse.dot(w),\n",
    "                'dot product of b_dense and w should equal dot product of b_sparse and w')\n",
    "testmti850.Test.assertTrue(a_sparse.numNonzeros() == 2, 'a_sparse should not store zero values')\n",
    "testmti850.Test.assertTrue(b_sparse.numNonzeros() == 1, 'b_sparse should not store zero values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) OHE features as sparse vectors\n",
    "\n",
    "Now let's see how we can represent the OHE features for points in our sample dataset.  Using the mapping defined by the OHE dictionary from Part (1a), manually define OHE features for the three sample data points using SparseVector format.  Any feature that occurs in a point should have the value 1.0.  For example, the `DenseVector` for a point with features 2 and 4 would be `[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]`. Following the table below, this vector represents `[(0, \"mouse\"), (1, \"tabby\")]`.\n",
    "\n",
    "| Feat. id | Feat. value | Onehot bit |\n",
    "| -------- | ----------- | ---------- |\n",
    "| 0        | bear        | 0          |\n",
    "| 0        | cat         | 1          |\n",
    "| 0        | mouse       | 2          |\n",
    "| 1        | black       | 3          |\n",
    "| 1        | tabby       | 4          |\n",
    "| 2        | mouse       | 5          |\n",
    "| 2        | salmon      | 6          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3 lines below are a reminder of the sample features\n",
    "# sample_one = [(0, 'mouse'), (1, 'black')]\n",
    "# sample_two = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "# sample_three =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]\n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "sample_one_ohe_feat_manual = SparseVector(7, [2,3], [1.0, 1.0])\n",
    "\n",
    "sample_two_ohe_feat_manual = SparseVector(7, [1,4,5], [1.0, 1.0, 1.0])\n",
    "\n",
    "sample_three_ohe_feat_manual = SparseVector(7, [0,3,6], [1.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST OHE Features as sparse vectors (1c)\n",
    "testmti850.Test.assertTrue(isinstance(sample_one_ohe_feat_manual, SparseVector),\n",
    "                'sample_one_ohe_feat_manual needs to be a SparseVector')\n",
    "testmti850.Test.assertTrue(isinstance(sample_two_ohe_feat_manual, SparseVector),\n",
    "                'sample_two_ohe_feat_manual needs to be a SparseVector')\n",
    "testmti850.Test.assertTrue(isinstance(sample_three_ohe_feat_manual, SparseVector),\n",
    "                'sample_three_ohe_feat_manual needs to be a SparseVector')\n",
    "testmti850.Test.assertEqualsHashed(sample_one_ohe_feat_manual,\n",
    "                        'ecc00223d141b7bd0913d52377cee2cf5783abd6',\n",
    "                        'incorrect value for sample_one_ohe_feat_manual')\n",
    "testmti850.Test.assertEqualsHashed(sample_two_ohe_feat_manual,\n",
    "                        '26b023f4109e3b8ab32241938e2e9b9e9d62720a',\n",
    "                        'incorrect value for sample_two_ohe_feat_manual')\n",
    "testmti850.Test.assertEqualsHashed(sample_three_ohe_feat_manual,\n",
    "                        'c04134fd603ae115395b29dcabe9d0c66fbdc8a7',\n",
    "                        'incorrect value for sample_three_ohe_feat_manual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d) Define a OHE function\n",
    "\n",
    "Next we will use the OHE dictionary from Part (1a) to programatically generate OHE features from the original categorical data.  First write a function called `one_hot_encoding` that creates OHE feature vectors in `SparseVector` format.  Then use this function to create OHE features for the first sample data point and verify that the result matches the result from Part (1c).\n",
    "\n",
    "> Note: We'll pass in the OHE dictionary as a [Broadcast](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.Broadcast.html?highlight=broadcast#pyspark.Broadcast) variable, which will greatly improve performance when we call this function as part of a UDF. **When accessing a broadcast variable, you _must_ use `.value`.** For instance: `ohe_dict_broadcast.value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'mouse'), (1, 'black')]\n",
      "(7,[2,3],[1.0,1.0])\n"
     ]
    }
   ],
   "source": [
    "# the commented line below may help you to accomplish this task\n",
    "sample_one = [(0, 'mouse'), (1, 'black')]\n",
    "# SparseVector(7, [(sample_ohe_dict_manual[x], 1) for x in sample_one]) # 7 = 3 + 2 + 2 features\n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "def one_hot_encoding(raw_feats, ohe_dict_broadcast, num_ohe_feats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sample_one)\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "        num_ohe_feats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length num_ohe_feats with indices equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    ohe_dict = ohe_dict_broadcast.value\n",
    "    indices = [ohe_dict[feat] for feat in raw_feats]\n",
    "    indices = sorted(indices)\n",
    "    values = [1.0] * len(indices)\n",
    "\n",
    "    return SparseVector(num_ohe_feats, indices, values)\n",
    "\n",
    "# Calculate the number of features in sample_ohe_dict_manual (varibable that encodes the table in Section 1c)\n",
    "num_sample_ohe_feats = len(sample_ohe_dict_manual)\n",
    "\n",
    "sample_ohe_dict_manual_broadcast = spark.sparkContext.broadcast(sample_ohe_dict_manual)\n",
    "\n",
    "# Run one_hot_encoding() on sample_one.  Make sure to pass in the Broadcast variable.\n",
    "sample_one_ohe_feat = one_hot_encoding(sample_one, sample_ohe_dict_manual_broadcast, num_sample_ohe_feats)\n",
    "\n",
    "print(sample_one)\n",
    "\n",
    "print(sample_one_ohe_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Define an OHE Function (1d)\n",
    "testmti850.Test.assertTrue(sample_one_ohe_feat == sample_one_ohe_feat_manual,\n",
    "                'sample_one_ohe_feat should equal sample_one_ohe_feat_manual')\n",
    "testmti850.Test.assertEquals(sample_one_ohe_feat, SparseVector(7, [2, 3], [1.0, 1.0]),\n",
    "                  'incorrect value for sample_one_ohe_feat')\n",
    "testmti850.Test.assertEquals(one_hot_encoding([(1, 'black'), (0, 'mouse')], sample_ohe_dict_manual_broadcast,\n",
    "                                   num_sample_ohe_feats), SparseVector(7, [2, 3], [1.0, 1.0]),\n",
    "                  'incorrect definition for one_hot_encoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1e) Apply OHE to a dataset\n",
    "\n",
    "Finally, use the function from Part (1d) to create OHE features for all three data points in the sample dataset.  You will need to generate a [UDF](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html?highlight=udf#pyspark.sql.functions.udf) that can be used in a `DataFrame` `select` statement.\n",
    "\n",
    "> Note: Your implemenation of `ohe_udf_generator` needs to call your `one_hot_encoding` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|ohe_features             |\n",
      "+-------------------------+\n",
      "|(7,[2,3],[1.0,1.0])      |\n",
      "|(7,[1,4,5],[1.0,1.0,1.0])|\n",
      "|(7,[0,3,6],[1.0,1.0,1.0])|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "def ohe_udf_generator(ohe_dict_broadcast):\n",
    "    \"\"\"Generate a UDF that is setup to one-hot-encode rows with the given dictionary.\n",
    "\n",
    "    Note:\n",
    "        We'll reuse this function to generate a UDF that can one-hot-encode rows based on a\n",
    "        one-hot-encoding dictionary built from the training data.  Also, you should calculate\n",
    "        the number of features before calling the one_hot_encoding function.\n",
    "\n",
    "    Args:\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "\n",
    "    Returns:\n",
    "        UserDefinedFunction: A UDF can be used in `DataFrame` `select` statement to call a\n",
    "            function on each row in a given column.  This UDF should call the one_hot_encoding\n",
    "            function with the appropriate parameters.\n",
    "    \"\"\"\n",
    "    length = len(ohe_dict_broadcast.value)\n",
    "    \n",
    "    return udf(lambda x: one_hot_encoding(x, ohe_dict_broadcast, length), VectorUDT())\n",
    "\n",
    "sample_ohe_dict_udf = ohe_udf_generator(sample_ohe_dict_manual_broadcast)\n",
    "\n",
    "sample_ohe_df = sample_data_df.select(sample_ohe_dict_udf(\"features\").alias(\"ohe_features\"))\n",
    "\n",
    "sample_ohe_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Apply OHE to a dataset (1e)\n",
    "sample_ohe_data_values = sample_ohe_df.collect()\n",
    "testmti850.Test.assertTrue(len(sample_ohe_data_values) == 3, 'sample_ohe_data_values should have three elements')\n",
    "testmti850.Test.assertEquals(sample_ohe_data_values[0], (SparseVector(7, {2: 1.0, 3: 1.0}),),\n",
    "                  'incorrect OHE for first sample')\n",
    "testmti850.Test.assertEquals(sample_ohe_data_values[1], (SparseVector(7, {1: 1.0, 4: 1.0, 5: 1.0}),),\n",
    "                  'incorrect OHE for second sample')\n",
    "testmti850.Test.assertEquals(sample_ohe_data_values[2], (SparseVector(7, {0: 1.0, 3: 1.0, 6: 1.0}),),\n",
    "                  'incorrect OHE for third sample')\n",
    "testmti850.Test.assertTrue('one_hot_encoding' in sample_ohe_dict_udf.func.__code__.co_names,\n",
    "                'ohe_udf_generator should call one_hot_encoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Construct an OHE dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) DataFrame with rows of `(featureID, category)`\n",
    "\n",
    "To start, create a DataFrame of distinct `(feature_id, category)` tuples. In our sample dataset, the seven items in the resulting DataFrame are:\n",
    "* `(0, 'bear')`,\n",
    "* `(0, 'cat')`, \n",
    "* `(0, 'mouse')`,\n",
    "* `(1, 'black')`,\n",
    "* `(1, 'tabby')`,\n",
    "* `(2, 'mouse')`,\n",
    "* `(2, 'salmon')`.\n",
    "\n",
    "Notably `'black'` appears twice in the dataset but only contributes one item to the DataFrame: `(1, 'black')`, while `'mouse'` also appears twice and contributes two items: `(0, 'mouse')` and `(2, 'mouse')`.  Use [explode](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.explode.html?highlight=explode#pyspark.sql.functions.explode) and [distinct](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.distinct.html?highlight=distinct#pyspark.sql.DataFrame.distinct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|featureID|category|\n",
      "+---------+--------+\n",
      "|0        |cat     |\n",
      "|0        |mouse   |\n",
      "|1        |black   |\n",
      "|2        |mouse   |\n",
      "|2        |salmon  |\n",
      "|0        |bear    |\n",
      "|1        |tabby   |\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "sample_distinct_feats_df_0 = sample_data_df.select(explode(\"features\").alias(\"feat\"))\n",
    "\n",
    "sample_distinct_feats_df = (sample_distinct_feats_df_0.select(\"feat._1\", \"feat._2\").distinct().toDF(\"featureID\", \"category\")\n",
    ")\n",
    "\n",
    "sample_distinct_feats_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) OHE Dictionary from distinct features\n",
    "\n",
    "Next, create an RDD of key-value tuples, where each `(feature_id, category)` tuple in `sample_distinct_feats_df` is a key and the values are distinct integers ranging from 0 to (number of keys - 1). Then convert this RDD into a dictionary, which can be done using the `collectAsMap` action.  Note that there is no unique mapping from keys to values, as all we require is that each `(featureID, category)` key be mapped to a unique integer between 0 and the number of keys.  In this exercise, any valid mapping is acceptable.  Use [zipWithIndex](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.zipWithIndex.html?highlight=zipwithindex#pyspark.RDD.zipWithIndex) followed by [collectAsMap](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collectAsMap.html?highlight=collectasmap#pyspark.RDD.collectAsMap).\n",
    "\n",
    "In our sample dataset, one valid list of key-value tuples is: `[((0, 'bear'), 0), ((2, 'salmon'), 1), ((1, 'tabby'), 2), ((2, 'mouse'), 3), ((0, 'mouse'), 4), ((0, 'cat'), 5), ((1, 'black'), 6)]`. The dictionary defined in Part (1a) illustrates another valid mapping between keys and integers.\n",
    "\n",
    "> Note: We provide the code to convert the DataFrame to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|featureID|category|\n",
      "+---------+--------+\n",
      "|        0|     cat|\n",
      "|        0|   mouse|\n",
      "|        1|   black|\n",
      "|        2|   mouse|\n",
      "|        2|  salmon|\n",
      "|        0|    bear|\n",
      "|        1|   tabby|\n",
      "+---------+--------+\n",
      "\n",
      "{(0, 'cat'): 0, (0, 'mouse'): 1, (1, 'black'): 2, (2, 'mouse'): 3, (2, 'salmon'): 4, (0, 'bear'): 5, (1, 'tabby'): 6}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "sample_distinct_feats_df.show()\n",
    "\n",
    "sample_ohe_dict = sample_ohe_dict = (sample_distinct_feats_df.rdd.map(lambda r: (r[0], r[1])).zipWithIndex().collectAsMap())\n",
    "# extraction du tuble marchait pas je l'ai réarrangée\n",
    "\n",
    "print(sample_ohe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST OHE Dictionary from distinct features (2b)\n",
    "testmti850.Test.assertEquals(\n",
    "    sorted(sample_ohe_dict.keys()),\n",
    "    [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'), (1, 'tabby'), (2, 'mouse'), (2, 'salmon')],\n",
    "    'sample_ohe_dict has unexpected keys'\n",
    ")\n",
    "testmti850.Test.assertEquals(sorted(sample_ohe_dict.values()), list(range(7)), 'sample_ohe_dict has unexpected values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Automated creation of an OHE dictionary\n",
    "\n",
    "Now use the code from Parts (2a) and (2b) to write a function that takes an input dataset and outputs an OHE dictionary.  Then use this function to create an OHE dictionary for the sample dataset, and verify that it matches the dictionary from Part (2b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 'cat'): 0, (0, 'mouse'): 1, (1, 'black'): 2, (2, 'mouse'): 3, (2, 'salmon'): 4, (0, 'bear'): 5, (1, 'tabby'): 6}\n",
      "+------------------------------------+\n",
      "|features                            |\n",
      "+------------------------------------+\n",
      "|[{0, mouse}, {1, black}]            |\n",
      "|[{0, cat}, {1, tabby}, {2, mouse}]  |\n",
      "|[{0, bear}, {1, black}, {2, salmon}]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "def create_one_hot_dict(input_df):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        input_df (DataFrame with 'features' column): A DataFrame where each row contains a list of\n",
    "            (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "    \"\"\"\n",
    "    \n",
    "    return (\n",
    "        input_df.select(explode(\"features\").alias(\"feat\")).select(\"feat._1\", \"feat._2\").distinct().rdd.map(lambda r: (r[0], r[1])).zipWithIndex().collectAsMap()\n",
    "    )\n",
    "\n",
    "\n",
    "sample_ohe_dict_auto = create_one_hot_dict(sample_data_df)\n",
    "\n",
    "print(sample_ohe_dict_auto)\n",
    "\n",
    "sample_data_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here ``(0, 'bear')`` maps to 5 instead of 0, as in the table in Section 1c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Automated creation of an OHE dictionary (2c)\n",
    "testmti850.Test.assertEquals(\n",
    "    sorted(sample_ohe_dict_auto.keys()),\n",
    "    [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'), (1, 'tabby'), (2, 'mouse'), (2, 'salmon')],\n",
    "    'sample_ohe_dict_auto has unexpected keys'\n",
    ")\n",
    "testmti850.Test.assertEquals(sorted(sample_ohe_dict_auto.values()), list(range(7)), 'sample_ohe_dict_auto has unexpected values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parse CTR data and generate OHE features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can proceed, you will first need to obtain the data from Criteo (or download directly from Moodle).  \n",
    "\n",
    "* Run the following command in a terminal shell cell to download the dataset and make it available in the hdfs filesystem.\n",
    "```\n",
    "start-dfs.sh && start-yarn.sh\n",
    "mkdir dac_sample\n",
    "wget http://labs.criteo.com/wp-content/uploads/2015/04/dac_sample.tar.gz\n",
    "mkdir dac_sample && tar -zxf dac_sample.tar.gz --directory dac_sample\n",
    "hdfs dfs -put dac_sample /dac_sample\n",
    "```\n",
    "* The next cell make available the dataset as a `DataFrame` in variable `raw_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, it removes the line separators characters (\\r and \\n)\n",
    "raw_df = spark.read.text(\"hdfs://localhost:9000/dac_sample/dac_sample.txt\").withColumnRenamed(\"value\", \"text\")\n",
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Loading and splitting the data\n",
    "\n",
    "We are now ready to start working with the actual CTR data, and our first task involves splitting it into training, validation, and test sets.\n",
    "\n",
    "Use the [randomSplit method](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.randomSplit.html?highlight=randomsplit#pyspark.sql.DataFrame.randomSplit) with the specified weights and seed to create DFs storing each of these datasets, and then [cache](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.cache.html?highlight=cache#pyspark.sql.DataFrame.cache) each of these DFs, as we will be accessing them multiple times in the remainder of this assignment.\n",
    "\n",
    "Finally, compute the size of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "\n",
    "# Use randomSplit with weights and seed (note that randomSplit perform sort within partitions)\n",
    "raw_train_df, raw_validation_df, raw_test_df = raw_df.<FILL_IN>\n",
    "\n",
    "# Cache and count the DataFrames\n",
    "n_train = raw_train_df.<FILL_IN>\n",
    "n_val = raw_validation_df.<FILL_IN>\n",
    "n_test = raw_test_df.<FILL_IN>\n",
    "\n",
    "print(\"Train samples = \"+str(n_train), \"\\nValidation samples = \"+str(n_val),\n",
    "      \"\\nTest samples = \"+str(n_test), \"\\nTotal number of samples = \"+str(n_train + n_val + n_test))\n",
    "\n",
    "raw_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Loading and splitting the data (3a)\n",
    "\n",
    "testmti850.Test.assertTrue(all([raw_train_df.is_cached, raw_validation_df.is_cached, raw_test_df.is_cached]),\n",
    "                'you must cache the split data')\n",
    "testmti850.Test.assertEquals(n_train, 79901, 'incorrect value for n_train')\n",
    "testmti850.Test.assertEquals(n_val, 10037, 'incorrect value for n_val')\n",
    "testmti850.Test.assertEquals(n_test, 10062, 'incorrect value for n_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Extract features\n",
    "\n",
    "We will now parse the raw training data to create a `DataFrame`  that we can subsequently use to create an OHE dictionary. Note from the `show()` command in Part (3a) that each raw data point is a string containing several fields separated by some delimiter.  For now, we will ignore the first field (which is the 0-1 label), and parse the remaining fields (or raw features).  To do this, complete the implemention of the `parse_point` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "def parse_point(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    \n",
    "    return <FILL_IN>\n",
    "\n",
    "pt = raw_df.select('text').first()[0]\n",
    "\n",
    "print(pt)\n",
    "\n",
    "print(parse_point(raw_df.select('text').first()[0])[: 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these features are similar to that involving animals. Rather than strings with animal names, now we have strings with numeric content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Extract features (3b)\n",
    "testmti850.Test.assertEquals(\n",
    "    parse_point(raw_df.select('text').first()[0])[: 3],\n",
    "    [(0, u'1'), (1, u'1'), (2, u'5')],\n",
    "    'incorrect implementation of parse_point'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c) Extracting features continued\n",
    "\n",
    "Next, we will create a `parse_raw_df` function that creates a `label` column from the first value in the text and a `feature` column from the rest of the values.  The `feature` column will be created using `parse_point_udf`, which we've provided and is based on your `parse_point` function.  Note that to name your columns you should use [alias](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.alias.html?highlight=column%20alias#pyspark.sql.Column.alias).\n",
    "\n",
    "You can split the `text` field in `raw_df` using [split](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.split.html?highlight=split#pyspark.sql.functions.split) and retrieve the first value of the resulting array with [getItem](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.getItem.html?highlight=column%20getitem#pyspark.sql.Column.getItem).\n",
    "\n",
    "Be sure to call [cast](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.cast.html?highlight=column%20cast#pyspark.sql.Column.cast) to cast the column value to `double`. Your `parse_raw_df` function should also cache the DataFrame it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with the appropriate code\n",
    "from pyspark.sql.functions import udf, split, monotonically_increasing_id\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, LongType, StringType\n",
    "\n",
    "parse_point_udf = udf(parse_point, ArrayType(StructType([StructField('_1', LongType()),\n",
    "                                                         StructField('_2', StringType())])))\n",
    "\n",
    "def parse_raw_df(raw_df):\n",
    "    \"\"\"Convert a DataFrame consisting of rows of tab separated text into labels and feature.\n",
    "\n",
    "    Args:\n",
    "        raw_df (DataFrame with a 'text' column): DataFrame containing the raw comma separated data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with 'label' and 'feature' columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remember that each row comprises a 0-or-1 label (item 0) and a sequence of numbers\n",
    "    return <FILL_IN>\n",
    "\n",
    "\n",
    "# Parse the raw training DataFrame\n",
    "parsed_train_df = <FILL_IN>\n",
    "\n",
    "from pyspark.sql.functions import explode, col\n",
    "num_categories = (parsed_train_df.select(explode('feature').alias('feature')).distinct()\n",
    "                    .select(col('feature').getField('_1').alias('featureNumber'))\n",
    "                    .groupBy('featureNumber')\n",
    "                    .sum()\n",
    "                    .orderBy('featureNumber')\n",
    "                    .collect())\n",
    "\n",
    "\n",
    "# show the number of categories associated with the featureNumber 2\n",
    "print(num_categories[2][1])\n",
    "\n",
    "print('first rows parsed_train_df')\n",
    "parsed_train_df.show(2, truncate=40)\n",
    "\n",
    "print('last rows of parsed_train_df')\n",
    "parsed_train_df_inverted = parsed_train_df.withColumn(\n",
    "    \"index\", monotonically_increasing_id()\n",
    ").sort('index', ascending=False).select('label', 'feature')\n",
    "parsed_train_df_inverted.show(2, truncate=40)\n",
    "\n",
    "raw_train_df.show(2, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first two rows have several features with blank/null value (e.g., [2, ], [3, ], [4, ])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Extract features (3c)\n",
    "testmti850.Test.assertTrue(parsed_train_df.is_cached, 'parse_raw_df should return a cached DataFrame')\n",
    "testmti850.Test.assertEquals(num_categories[2][1], 1702, 'incorrect implementation of parse_point or parse_raw_df')\n",
    "testmti850.Test.assertEquals(num_categories[32][1], 128, 'incorrect implementation of parse_point or parse_raw_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Create an OHE dictionary from the dataset\n",
    "\n",
    "Note that `parse_point` returns a data point as a list of `(featureID, category)` tuples, which is the same format as the sample dataset studied in Parts 1 and 2 of this assignment.  Using this observation, create an OHE dictionary from the parsed training data using the function implemented in Part (2c).\n",
    "\n",
    "Note that we will assume for simplicity that all features in our CTR dataset are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "ctr_ohe_dict = <FILL_IN>\n",
    "\n",
    "num_ctr_ohe_feats = l<FILL_IN>\n",
    "\n",
    "print(num_ctr_ohe_feats)\n",
    "\n",
    "print(ctr_ohe_dict[(0, '')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Create an OHE dictionary from the dataset (3d)\n",
    "testmti850.Test.assertEquals(num_ctr_ohe_feats, 234658, 'incorrect number of features in ctr_ohe_dict')\n",
    "testmti850.Test.assertTrue((0, '') in ctr_ohe_dict, 'incorrect features in ctr_ohe_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3e) Apply OHE to the dataset\n",
    "\n",
    "Now let's use this OHE dictionary, by starting with the training data that we've parsed into `label` and `feature` columns, to create one-hot-encoded features.\n",
    "\n",
    "Recall that we created a function `ohe_udf_generator` that can create the UDF that we need to convert row into `features`.  Make sure that `ohe_train_df` contains a `label` and `features` column and is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with the appropriate code\n",
    "\n",
    "ohe_dict_broadcast = <FILL_IN>\n",
    "\n",
    "ohe_dict_udf = <FILL_IN>\n",
    "\n",
    "# each row of parsed_train_df is a list, the suitable format for ohe_dict_udf. a dict is\n",
    "# generated for each row and gives rise to a SparseVector\n",
    "ohe_train_df = <FILL_IN>\n",
    "\n",
    "print(ohe_train_df.count())\n",
    "\n",
    "print(ohe_train_df.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Apply OHE to the dataset (3e)\n",
    "testmti850.Test.assertTrue('label' in ohe_train_df.columns and 'features' in ohe_train_df.columns, 'ohe_train_df should have label and features columns')\n",
    "testmti850.Test.assertTrue(ohe_train_df.is_cached, 'ohe_train_df should be cached')\n",
    "num_nz = sum(parsed_train_df.rdd.map(lambda r: len(r[1])).take(5))\n",
    "num_nz_alt = sum(ohe_train_df.rdd.map(lambda r: len(r[1].indices)).take(5))\n",
    "testmti850.Test.assertEquals(num_nz, num_nz_alt, 'incorrect value for ohe_train_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 1: Feature frequency\n",
    "\n",
    "We will now visualize the number of times each of the 234,658 OHE features appears in the training data. We first compute the number of times each feature appears, then bucket the features by these counts.\n",
    "\n",
    "The buckets are sized by powers of 2, so the first bucket corresponds to features that appear exactly once ( \\\\( \\scriptsize 2^0 \\\\) ), the second to features that appear twice ( \\\\( \\scriptsize 2^1 \\\\) ), the third to features that occur between three and four ( \\\\( \\scriptsize 2^2 \\\\) ) times, the fifth bucket is five to eight ( \\\\( \\scriptsize 2^3 \\\\) ) times and so on.\n",
    "\n",
    "The scatter plot below shows the logarithm of the bucket thresholds versus the logarithm of the number of features that have counts that fall in the buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.sql.functions import log2\n",
    "\n",
    "# indices of the 1's in the one-hot encoded features\n",
    "get_indices = udf(lambda sv: list(map(int, sv.indices)), ArrayType(IntegerType()))\n",
    "feature_counts_df = (ohe_train_df\n",
    "                    .select(explode(get_indices('features')))\n",
    "                    .groupBy('col')\n",
    "                    .count()\n",
    "                    .withColumn('bucket', log2('count').cast('int'))\n",
    "                    .groupBy('bucket')\n",
    "                    .count()\n",
    "                    .orderBy('bucket'))\n",
    "                 \n",
    "feature_counts_df.show()\n",
    "\n",
    "feature_counts = feature_counts_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, y = zip(*feature_counts) # bucket x count\n",
    "x, y = x, np.log2(y)\n",
    "\n",
    "def prepare_plot(xticks, yticks, figsize=(10.5, 6), hide_labels=False, grid_color='#999999',\n",
    "                 grid_width=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    ax.set_xlim([-.5, xticks[-1] + .5])\n",
    "    ax.set_ylim([-.5, yticks[-1]])\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hide_labels: axis.set_ticklabels([])\n",
    "    plt.grid(color=grid_color, linewidth=grid_width, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, max(x) + 1, 1), np.arange(0, max(y) + 1, 2))\n",
    "ax.set_xlabel(r'$\\log_2(bucketSize)$')\n",
    "ax.set_ylabel(r'$\\log_2(countInBucket)$')\n",
    "ax.scatter(x, y, s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3f) Handling unseen features\n",
    "\n",
    "We naturally would like to repeat the process from Part (3e), to compute OHE features for the validation and test datasets.  However, we must be careful, as some categorical values will likely appear in new data that did not exist in the training data. To deal with this situation, update the `one_hot_encoding()` function from Part (1d) to ignore previously unseen categories, and then compute OHE features for the validation data.  Remember that you can parse a raw DataFrame using `parse_raw_df`.\n",
    "\n",
    "> Note: you will have to generate a new UDF using `ohe_udf_generator` so that the updated `one_hot_encoding` function is used.  And make sure to cache `ohe_validation_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "def one_hot_encoding(raw_feats, ohe_dict_broadcast, num_ohe_feats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted, and that the\n",
    "        function handles missing features.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sample_one)\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "        num_ohe_feats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length num_ohe_feats with indices equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    return <FILL_IN>\n",
    "\n",
    "ohe_dict_missing_udf = <FILL_IN>\n",
    "\n",
    "ohe_validation_df = ( <FILL_IN> )\n",
    "\n",
    "ohe_validation_df.count()\n",
    "\n",
    "ohe_validation_df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Handling unseen features (3f)\n",
    "from pyspark.sql.functions import size, sum as sqlsum\n",
    "\n",
    "testmti850.Test.assertTrue(ohe_validation_df.is_cached, 'you need to cache ohe_validation_df')\n",
    "num_nz_val = (ohe_validation_df\n",
    "                .select(sqlsum(size(get_indices('features'))))\n",
    "                .first()[0])\n",
    "\n",
    "nz_expected = 371557\n",
    "testmti850.Test.assertEquals(num_nz_val, nz_expected, 'incorrect number of features: Got {0}, expected {1}'.format(num_nz_val, nz_expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: CTR prediction and logloss evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4a) Logistic regression\n",
    "\n",
    "We are now ready to train our first CTR classifier. A natural classifier to use in this setting is logistic regression, since it models the probability of a click-through event rather than returning a binary response, and when working with rare events, probabilistic predictions are useful.\n",
    "\n",
    "First use [LogisticRegression](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html?highlight=logistic#pyspark.ml.classification.LogisticRegression) from the pyspark.ml package to train a model using `ohe_train_df` with the given hyperparameter configuration. `LogisticRegression.fit` returns a [LogisticRegressionModel](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegressionModel.html?highlight=logistic#pyspark.ml.classification.LogisticRegressionModel).\n",
    "\n",
    "Next, we will use the `LogisticRegressionModel.coefficients` and `LogisticRegressionModel.intercept` attributes to print out some details of the model's parameters.\n",
    "\n",
    "Note that these are the names of the object's attributes and should be called using a syntax like `model.coefficients` for a given `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "standardization = False\n",
    "elastic_net_param = 0.0\n",
    "reg_param = .01\n",
    "max_iter = 20\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = <FILL_IN>\n",
    "\n",
    "lr_model_basic =  <FILL_IN>\n",
    "\n",
    "print('intercept: {0}'.format(lr_model_basic.intercept))\n",
    "\n",
    "print('length of coefficients: {0}'.format(len(lr_model_basic.coefficients)))\n",
    "\n",
    "sorted_coefficients = sorted(lr_model_basic.coefficients)[:5]\n",
    "\n",
    "print(sorted_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Logistic regression (4a)\n",
    "\n",
    "# Values updated for Spark 4. [A25]\n",
    "testmti850.Test.assertTrue(np.allclose(lr_model_basic.intercept, -1.17759725244514), 'incorrect value for model intercept')\n",
    "testmti850.Test.assertTrue(np.allclose(sorted_coefficients, [-0.11267143173158842, -0.10163425803646528, -0.10063997333018701, -0.10045027889726595, -0.09975078904287121]), 'incorrect value for model coefficients')\n",
    "\n",
    "# Old tests for Spark 3. \n",
    "#testmti850.Test.assertTrue(np.allclose(lr_model_basic.intercept, -1.2182610216249579), 'incorrect value for model intercept')\n",
    "#testmti850.Test.assertTrue(np.allclose(sorted_coefficients, [-0.11580845900604468, -0.10500848917554309, -0.10394334471145178, -0.10374477431186264, -0.10289202894898541]), 'incorrect value for model coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## (4b) Log loss\n",
    "Throughout this assignment, we will use log loss to evaluate the quality of models.\n",
    "\n",
    "Log loss is defined as: \\\\[ \\scriptsize \\ell_{log}(p, y) = \\begin{cases} -\\log (p) & \\text{if } y = 1 \\\\\\ -\\log(1-p) & \\text{if } y = 0 \\end{cases} \\\\]\n",
    "\n",
    "where \\\\( \\scriptsize p\\\\) is a probability between 0 and 1 and \\\\( \\scriptsize y\\\\) is a label of either 0 or 1. \n",
    "\n",
    "Log loss is a standard evaluation criterion when predicting rare-events such as click-through rate prediction (it is also the criterion used in the [Criteo Kaggle competition](https://www.kaggle.com/c/criteo-display-ad-challenge)).\n",
    "\n",
    "Write a function `add_log_loss` to a DataFrame, and evaluate it on some sample inputs.  This does not require a UDF.  You can perform conditional branching with DataFrame columns using [when](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.when.html?highlight=when#pyspark.sql.Column.when)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some example data\n",
    "example_log_loss_df = spark.createDataFrame(\n",
    "    [(.5, 1), (.5, 0), (.99, 1), (.99, 0), (.01, 1), (.01, 0), (1., 1), (.0, 1), (1., 0)], # data\n",
    "    ['p', 'label'] # columns\n",
    ")\n",
    "example_log_loss_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "from pyspark.sql.functions import when, log, col\n",
    "\n",
    "epsilon = 1e-16\n",
    "\n",
    "def add_log_loss(df):\n",
    "    \"\"\"Computes and adds a 'log_loss' column to a DataFrame using 'p' and 'label' columns.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we add a small value (epsilon) to it and when\n",
    "        p is 1 we subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'p' and 'label' columns): A DataFrame with a probability column\n",
    "            'p' and a 'label' column that corresponds to y in the log loss formula.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with an additional column called 'log_loss' where 'log_loss'\n",
    "        column contains the loss value as explained above.\n",
    "    \"\"\"\n",
    "    return d<FILL_IN>\n",
    "\n",
    "add_log_loss(example_log_loss_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Log loss (4b)\n",
    "log_loss_values = add_log_loss(example_log_loss_df).select('log_loss').rdd.map(lambda r: r[0]).collect()\n",
    "testmti850.Test.assertTrue(np.allclose(log_loss_values[:-2],\n",
    "                            [0.6931471805599451, 0.6931471805599451, 0.010050335853501338, 4.60517018598808,\n",
    "                             4.605170185988081, 0.010050335853501338, -0.0]), 'log loss is not correct')\n",
    "testmti850.Test.assertTrue(not(any(map(lambda x: x is None, log_loss_values[-2:]))),\n",
    "                'log loss needs to bound p away from 0 and 1 by epsilon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4c)  Baseline log loss\n",
    "\n",
    "Next we will use the function we wrote in Part (4b) to compute the baseline log loss on the training data.\n",
    "\n",
    "A very simple yet natural baseline model is one where we always make the same prediction independent of the given datapoint, setting the predicted value equal to the fraction of training points that correspond to click-through events (i.e., where the label is one).\n",
    "\n",
    "Compute this value (which is simply the mean of the training labels), and then use it to compute the training log loss for the baseline model.\n",
    "\n",
    "> Note: you'll need to add a `p` column to the `ohe_train_df` DataFrame so that it can be used in your function from Part (4b).  To represent a constant value as a column you can use the [lit](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.lit.html?highlight=lit#pyspark.sql.functions.lit) function to wrap the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Note that our dataset has a very high click-through rate by design\n",
    "# In practice click-through rate can be one to two orders of magnitude lower\n",
    "\n",
    "from pyspark.sql.functions import lit # creates a column with literal value\n",
    "\n",
    "class_one_frac_train = <FILL_IN>\n",
    "print('Training class one fraction = {0:.3f}'.format(class_one_frac_train))\n",
    "\n",
    "log_loss_tr_base = (<FILL_IN>\n",
    "print('Baseline Train Logloss = {0:.3f}\\n'.format(log_loss_tr_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Baseline log loss (4c)\n",
    "expected_frac = 0.22835759252074442\n",
    "expected_log_loss = 0.5372841736106333\n",
    "\n",
    "testmti850.Test.assertTrue(np.allclose(class_one_frac_train, expected_frac), 'incorrect value for class_one_frac_train. Got {0}, expected {1}'.format(class_one_frac_train, expected_frac))\n",
    "testmti850.Test.assertTrue(np.allclose(log_loss_tr_base, expected_log_loss), 'incorrect value for log_loss_tr_base. Got {0}, expected {1}'.format(log_loss_tr_base, expected_log_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4d) Predicted probability\n",
    "\n",
    "In order to compute the log loss for the model we trained in Part (4a), we need to write code to generate predictions from this model. Write a function that computes the raw linear prediction from this logistic regression model and then passes it through a [sigmoid function](http://en.wikipedia.org/wiki/Sigmoid_function) \\\\( \\scriptsize \\sigma(t) = (1+ e^{-t})^{-1} \\\\) to return the model's probabilistic prediction. Then compute probabilistic predictions on the training data.\n",
    "\n",
    "Note that when incorporating an intercept into our predictions, we simply add the intercept to the value of the prediction obtained from the weights and features.  Alternatively, if the intercept was included as the first weight, we would need to add a corresponding feature to our data where the feature has the value one.  This is not the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.sql.types import DoubleType\n",
    "from math import exp #  exp(-t) = e^-t\n",
    "\n",
    "def add_probability(df, model):\n",
    "    \"\"\"Adds a probability column ('p') to a DataFrame given a model\"\"\"\n",
    "    \n",
    "    coefficients_broadcast = spark.sparkContext.broadcast(model.coefficients)\n",
    "    \n",
    "    intercept = model.intercept\n",
    "\n",
    "    def get_p(features):\n",
    "        \"\"\"Calculate the probability for an observation given a list of features.\n",
    "\n",
    "        Note:\n",
    "            We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "        Args:\n",
    "            features: the features\n",
    "\n",
    "        Returns:\n",
    "            float: A probability between 0 and 1.\n",
    "        \"\"\"\n",
    "        # Compute the raw value\n",
    "        raw_prediction = <FILL_IN>\n",
    "        \n",
    "        # Bound the raw value between 20 and -20\n",
    "        raw_prediction = <FILL_IN>\n",
    "        \n",
    "        # Return the probability\n",
    "        \n",
    "        return <FILL_IN>\n",
    "\n",
    "    get_p_udf = udf(get_p, DoubleType())\n",
    "    \n",
    "    return df.withColumn('p', get_p_udf('features'))\n",
    "\n",
    "add_probability_model_basic = lambda df: add_probability(df, lr_model_basic)\n",
    "\n",
    "training_predictions = add_probability_model_basic(ohe_train_df).cache()\n",
    "\n",
    "training_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Predicted probability (4d)\n",
    "\n",
    "# Updated to Spark 4. [A25]\n",
    "expected = 19301.06209855696\n",
    "\n",
    "# Old value for Spark 3.\n",
    "# expected = 18333.7950991934\n",
    "\n",
    "got = training_predictions.selectExpr('sum(p)').first()[0]\n",
    "testmti850.Test.assertTrue(np.allclose(got, expected),\n",
    "                'incorrect value for training_predictions. Got {0}, expected {1}'.format(got, expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4e) Evaluate the model\n",
    "\n",
    "We are now ready to evaluate the quality of the model we trained in Part (4a). To do this, first write a general function that takes as input a model and data, and outputs the log loss.\n",
    "\n",
    "Note that the log loss for multiple observations is the mean of the individual log loss values. Then run this function on the OHE training data, and compare the result with the baseline log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "def evaluate_results(df, model, baseline = None):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Note:\n",
    "        If baseline has a value the probability should be set to baseline before\n",
    "        the log loss is calculated.  Otherwise, use add_probability to add the\n",
    "        appropriate probabilities to the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'label' and 'features' columns): A DataFrame containing\n",
    "            labels and features.\n",
    "        model (LogisticRegressionModel): A trained logistic regression model. This\n",
    "            can be None if baseline is set.\n",
    "        baseline (float): A baseline probability to use for the log loss calculation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "\n",
    "    with_probability_df = <FILL_IN>\n",
    "\n",
    "    with_log_loss_df = <FILL_IN>\n",
    "    \n",
    "    log_loss = <FILL_IN>\n",
    "    \n",
    "    return log_loss\n",
    "\n",
    "log_loss_train_model_basic = evaluate_results(ohe_train_df, lr_model_basic)\n",
    "\n",
    "print ('OHE Features Train Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n",
    "       .format(log_loss_tr_base, log_loss_train_model_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Evaluate the model (4e)\n",
    "\n",
    "# Updated value for Spark 4. [A25]\n",
    "expected_log_loss = 0.4783396327932997\n",
    "\n",
    "# Old values for Spark 3.\n",
    "# expected_log_loss = 0.4772118368120353\n",
    "\n",
    "testmti850.Test.assertTrue(np.allclose(log_loss_train_model_basic, expected_log_loss),\n",
    "                'incorrect value for log_loss_train_model_basic. Got {0}, expected {1}'.format(log_loss_train_model_basic, expected_log_loss))\n",
    "expected_res = 0.693147180558829\n",
    "res = evaluate_results(ohe_train_df, None,  0.5)\n",
    "testmti850.Test.assertTrue(np.allclose(res, expected_res),\n",
    "                'evaluate_results needs to handle baseline models. Got {0}, expected {1}'.format(res, expected_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4f) Validation log loss\n",
    "\n",
    "Next, using the `evaluate_results` function compute the validation log loss for both the baseline and logistic regression models.\n",
    "\n",
    "Notably, the baseline model for the validation data should still be based on the label fraction from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "log_loss_val_base = <FILL_IN>\n",
    "\n",
    "log_loss_val_l_r0 = <FILL_IN>\n",
    "\n",
    "print ('OHE Features Validation Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n",
    "       .format(log_loss_val_base, log_loss_val_l_r0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Validation log loss (4f)\n",
    "expected_val_base = 0.5256351707557996\n",
    "\n",
    "testmti850.Test.assertTrue(np.allclose(log_loss_val_base, expected_val_base),\n",
    "                'incorrect value for log_loss_val_base. Got {0}, expected {1}'.format(log_loss_val_base, expected_val_base))\n",
    "\n",
    "# Value updated for Spark 4. [A25]\n",
    "expected_val_model_basic = 0.4711432904191549\n",
    "\n",
    "# Old value for Spark 3.\n",
    "# expected_val_model_basic = 0.4692840176843362\n",
    "\n",
    "testmti850.Test.assertTrue(np.allclose(log_loss_val_l_r0, expected_val_model_basic),\n",
    "                'incorrect value for log_loss_val_l_r0. Got {0}, expected {1}'.format(log_loss_val_l_r0, expected_val_model_basic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 2: ROC curve\n",
    "\n",
    "We will now visualize how well the model predicts our target. To do this we generate a plot of the ROC curve.\n",
    "\n",
    "The ROC curve shows us the trade-off between the false positive rate and true positive rate, as we liberalize the threshold required to predict a positive outcome.\n",
    "\n",
    "A random model is represented by the dashed line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_and_scores = add_probability_model_basic(ohe_validation_df).select('label', 'p')\n",
    "labels_and_weights = labels_and_scores.collect()\n",
    "labels_and_weights.sort(key=lambda kv: kv[1], reverse=True)\n",
    "labels_by_weight = np.array([k for (k, v) in labels_and_weights])\n",
    "\n",
    "length = labels_by_weight.size\n",
    "true_positives = labels_by_weight.cumsum()\n",
    "num_positive = true_positives[-1]\n",
    "false_positives = np.arange(1.0, length + 1, 1.) - true_positives\n",
    "\n",
    "true_positive_rate = true_positives / num_positive\n",
    "false_positive_rate = false_positives / (length - num_positive)\n",
    "\n",
    "# Generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1))\n",
    "ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.plot(false_positive_rate, true_positive_rate, color='#8cbfd0', linestyle='-', linewidth=3.)\n",
    "plt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)  # Baseline model\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Reduce feature dimension via feature hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5a) Hash function\n",
    "\n",
    "As we just saw, using a one-hot-encoding (OHE) featurization can yield a model with good statistical accuracy.  However, the number of distinct categories across all features is quite large -- recall that we observed 234k categories in the training data in Part (3c).\n",
    "\n",
    "Moreover, the full Kaggle training dataset includes more than 33M distinct categories, and the Kaggle dataset itself is just a small subset of Criteo's labeled data. Hence, featurizing via a OHE representation would lead to a very large feature vector. To reduce the dimensionality of the feature space, we will use feature hashing.\n",
    "\n",
    "Below is the hash function that we will use for this part of the assignment. We will first use this hash function with the three sample data points from Part (1a) to gain some intuition.  Specifically, run code to hash the three sample points using two different values for `numBuckets` and observe the resulting hashed feature dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "def hash_function(raw_feats, num_buckets, print_mapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use print_mapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        num_buckets (int): Number of buckets to use as features.\n",
    "        print_mapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    encode = lambda s: s.encode('utf-8')\n",
    "    \n",
    "    mapping = { category + ':' + str(ind):\n",
    "                int(int(hashlib.md5((category + ':' + str(ind)).encode('utf-8')).hexdigest(), 16) % num_buckets)\n",
    "                for ind, category in raw_feats}\n",
    "    \n",
    "    if(print_mapping): print(mapping)\n",
    "    \n",
    "    sparse_features = defaultdict(float)\n",
    "    \n",
    "    for value in mapping.values():\n",
    "        sparse_features[value] += 1\n",
    "    \n",
    "    return dict(sparse_features)\n",
    "\n",
    "# Reminder of the sample values:\n",
    "# sample_one = [(0, 'mouse'), (1, 'black')]\n",
    "# sample_two = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "# sample_three =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# Use four buckets\n",
    "samp_one_four_buckets = hash_function(sample_one, <FILL_IN>, True)\n",
    "samp_two_four_buckets = hash_function(sample_two, <FILL_IN>,  True)\n",
    "samp_three_four_buckets = hash_function(sample_three, <FILL_IN>, True)\n",
    "\n",
    "# Use one hundred buckets\n",
    "samp_one_hundred_buckets = hash_function(sample_one, <FILL_IN>, True)\n",
    "samp_two_hundred_buckets = hash_function(sample_two, <FILL_IN>, True)\n",
    "samp_three_hundred_buckets = hash_function(sample_three, <FILL_IN>, True)\n",
    "\n",
    "print('\\n\\t\\t 4 Buckets \\t\\t  100 Buckets')\n",
    "print('SampleOne:\\t {0}\\t\\t  {1}'.format(samp_one_four_buckets, samp_one_hundred_buckets))\n",
    "print('SampleTwo:\\t {0}\\t  {1}'.format(samp_two_four_buckets, samp_two_hundred_buckets))\n",
    "print('SampleThree:\\t {0} {1}'.format(samp_three_four_buckets, samp_three_hundred_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Hash function (5a)\n",
    "testmti850.Test.assertEquals(samp_one_four_buckets, {3: 2.0}, 'incorrect value for samp_one_four_buckets')\n",
    "testmti850.Test.assertEquals(samp_three_hundred_buckets, {80: 1.0, 82: 1.0, 51: 1.0},\n",
    "                  'incorrect value for samp_three_hundred_buckets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5b) Creating hashed features\n",
    "\n",
    "Next we will use this hash function to create hashed features for our CTR datasets. Use the provided UDF to create a function that takes in a DataFrame and returns labels and hashed features.  Then use this function to create new training, validation and test datasets with hashed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "num_hash_buckets = 2 ** 15\n",
    "\n",
    "# UDF that returns a vector of hashed features given an Array of tuples\n",
    "tuples_to_hash_features_udf = udf(lambda x: Vectors.sparse(num_hash_buckets,\n",
    "                                                           hash_function(x, num_hash_buckets)),VectorUDT())\n",
    "\n",
    "def add_hashed_features(df):\n",
    "    \"\"\"Return a DataFrame with labels and hashed features.\n",
    "\n",
    "    Note:\n",
    "        Make sure to cache the DataFrame that you are returning.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'tuples' column): A DataFrame containing the tuples to be hashed.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with a 'label' column and a 'features' column that contains a\n",
    "            SparseVector of hashed features.\n",
    "    \"\"\"    \n",
    "    return <FILL_IN>\n",
    "    \n",
    "hash_train_df = <FILL_IN>\n",
    "\n",
    "hash_validation_df = <FILL_IN>\n",
    "\n",
    "hash_test_df = <FILL_IN>\n",
    "\n",
    "hash_train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Creating hashed features (5b)\n",
    "hash_train_df_feature_sum = sum(hash_train_df\n",
    "                                  .rdd\n",
    "                                  .map(lambda r: sum(r[1].indices))\n",
    "                                  .take(10))\n",
    "hash_validation_df_feature_sum = sum(hash_validation_df\n",
    "                                       .rdd\n",
    "                                       .map(lambda r: sum(r[1].indices))\n",
    "                                       .take(10))\n",
    "hash_test_df_feature_sum = sum(hash_test_df\n",
    "                                 .rdd\n",
    "                                 .map(lambda r: sum(r[1].indices))\n",
    "                                 .take(10))\n",
    "\n",
    "expected_train_sum = 6569082\n",
    "testmti850.Test.assertEquals(hash_train_df_feature_sum, expected_train_sum,\n",
    "                  'incorrect number of features in hash_train_df. Got {0}, expected {1}'.format(hash_train_df_feature_sum, expected_train_sum))\n",
    "\n",
    "expected_validation_sum = 6923755\n",
    "testmti850.Test.assertEquals(hash_validation_df_feature_sum, expected_validation_sum,\n",
    "                  'incorrect number of features in hash_validation_df. Got {0}, expected {1}'.format(hash_validation_df_feature_sum, expected_validation_sum))\n",
    "\n",
    "expected_test_sum = 6635530\n",
    "testmti850.Test.assertEquals(hash_test_df_feature_sum, expected_test_sum, 'incorrect number of features in hash_test_df. Got {0}, expected {1}'.format(hash_test_df_feature_sum, expected_test_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5c) Sparsity\n",
    "\n",
    "Since we have 33k hashed features versus 233k OHE features, we should expect OHE features to be sparser. Verify this hypothesis by computing the average sparsity of the OHE and the hashed training datasets.\n",
    "\n",
    "Note that if you have a `SparseVector` named `sparse`, calling `len(sparse)` returns the total number of features, not the number features with entries.  `SparseVector` objects have the attributes `indices` and `values` that contain information about which features are nonzero.\n",
    "\n",
    "Continuing with our example, these can be accessed using `sparse.indices` and `sparse.values`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def vector_feature_sparsity(sparse_vector):\n",
    "    \"\"\"Calculates the sparsity of a SparseVector.\n",
    "\n",
    "    Args:\n",
    "        sparse_vector (SparseVector): The vector containing the features.\n",
    "\n",
    "    Returns:\n",
    "        float: The ratio of features found in the vector to the total number of features.\n",
    "    \"\"\"\n",
    "    return <FILL_IN>\n",
    "\n",
    "feature_sparsity_udf = udf(vector_feature_sparsity, DoubleType())\n",
    "\n",
    "a_sparse_vector = Vectors.sparse(5, {0: 1.0, 3: 1.0})\n",
    "\n",
    "a_sparse_vector_sparsity = vector_feature_sparsity(a_sparse_vector)\n",
    "\n",
    "print('This vector should have sparsity 2/5 or .4.')\n",
    "\n",
    "print('Sparsity = {0:.2f}.'.format(a_sparse_vector_sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Sparsity (5c)\n",
    "testmti850.Test.assertEquals(a_sparse_vector_sparsity, .4,\n",
    "                'incorrect value for a_sparse_vector_sparsity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5d) Sparsity continued\n",
    "\n",
    "Now that we have a function to calculate vector sparsity, we will wrap it in a UDF and apply it to an entire DataFrame to obtain the average sparsity for features in that DataFrame.\n",
    "\n",
    "We will use the function to find the average sparsity of the one-hot-encoded training DataFrame and of the hashed training DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "feature_sparsity_udf = udf(vector_feature_sparsity, DoubleType())\n",
    "\n",
    "def get_sparsity(df):\n",
    "    \"\"\"Calculates the average sparsity for the features in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'features' column): A DataFrame with sparse features.\n",
    "\n",
    "    Returns:\n",
    "        float: The average feature sparsity.\n",
    "    \"\"\"\n",
    "    return <FILL_IN>\n",
    "\n",
    "average_sparsity_ohe = <FILL_IN>\n",
    "\n",
    "average_sparsity_hash = <FILL_IN>\n",
    "\n",
    "print('Average OHE Sparsity: {0:.7e}'.format(average_sparsity_ohe))\n",
    "\n",
    "print('Average Hash Sparsity: {0:.7e}'.format(average_sparsity_hash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Sparsity (5d)\n",
    "expected_ohe = 1.6619932e-04\n",
    "testmti850.Test.assertTrue(np.allclose(average_sparsity_ohe, expected_ohe),\n",
    "                'incorrect value for average_sparsity_ohe. Got {0}, expected {1}'.format(average_sparsity_ohe, expected_ohe))\n",
    "expected_hash = 1.1896642e-03\n",
    "testmti850.Test.assertTrue(np.allclose(average_sparsity_hash, expected_hash),\n",
    "                'incorrect value for average_sparsity_hash. Got {0}, expected {1}'.format(average_sparsity_hash, expected_hash))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5e) Logistic model with hashed features\n",
    "\n",
    "Now let's train a logistic regression model using the hashed training features. Use the hyperparameters provided, fit the model, and then evaluate the log loss on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "standardization = False\n",
    "elastic_net_param = 0.7\n",
    "reg_param = .001\n",
    "max_iter = 20\n",
    "\n",
    "lr_hash = <FILL_IN>\n",
    "\n",
    "lr_model_hashed = <FILL_IN>\n",
    "\n",
    "print('intercept: {0}'.format(lr_model_hashed.intercept))\n",
    "\n",
    "print(len(lr_model_hashed.coefficients))\n",
    "\n",
    "log_loss_train_model_hashed = <FILL_IN>\n",
    "\n",
    "print ('OHE Features Train Logloss:\\n\\tBaseline = {0:.3f}\\n\\thashed = {1:.3f}'\n",
    "       .format(log_loss_tr_base, log_loss_train_model_hashed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Logistic model with hashed features (5e)\n",
    "\n",
    "# Update values for Spark 4. [A25]\n",
    "expected = 0.4678969394413947\n",
    "\n",
    "# Old value for Spark 3.\n",
    "# expected = 0.468052083881297\n",
    "\n",
    "testmti850.Test.assertTrue(np.allclose(log_loss_train_model_hashed, expected),\n",
    "                'incorrect value for log_loss_train_model_hashed. Got {0}, expected {1}'.format(log_loss_train_model_hashed, expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5f) Evaluate on the test set\n",
    "\n",
    "Finally, evaluate the model from Part (5e) on the test set.  Compare the resulting log loss with the baseline log loss on the test set, which can be computed in the same way that the validation log loss was computed in Part (4f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Log loss for the best model from (5e)\n",
    "\n",
    "log_loss_test = <FILL_IN>\n",
    "\n",
    "# Log loss for the baseline model\n",
    "class_one_frac_test = <FILL_IN>\n",
    "\n",
    "print('Class one fraction for test data: {0}'.format(class_one_frac_test))\n",
    "\n",
    "log_loss_test_baseline = <FILL_IN>\n",
    "\n",
    "print ('Hashed Features Test Log Loss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n",
    "       .format(log_loss_test_baseline, log_loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Evaluate on the test set (5f)\n",
    "expected_test_baseline = 0.5278321942418345\n",
    "testmti850.Test.assertTrue(np.allclose(log_loss_test_baseline, expected_test_baseline),\n",
    "                'incorrect value for log_loss_test_baseline. Got {0}, expected {1}'.format(log_loss_test_baseline, expected_test_baseline))\n",
    "\n",
    "# Old values for Spark 4. [A25]\n",
    "expected_test = 0.45898741847256846\n",
    "\n",
    "# Old values for Spark 3.\n",
    "# expected_test = 0.4590996248004832\n",
    "\n",
    "testmti850.Test.assertTrue(np.allclose(log_loss_test, expected_test),\n",
    "                'incorrect value for log_loss_test. Got {0}, expected {1}'.format(log_loss_test, expected_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Ended"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
